# custom_nnets
### semestral project for fi muni pv021 course: neural networks
### author: Dominik Tuchyna

## update
This repo went public during the corona outbreak. I just suppose that my boredom levels have just raised up.

## about
this is a java implementation (i know, yuck) of the MLP nnet (i know even worse yuck) that is fine-tuned to be at least 95% successful on the test MNIST dataset

### architercture
first layer is only a TODO

### training
backpropagation is used to compute the gradient
stochastic gradient descent (SGD) is used in training process
momentum available
learning rate decay used (however it is not much generally usable, i would ofc change it to something more fancy as RMSProp or even better - Adam)



## trivia
overall the code was written in like one week or so, then overall refactored in one week or so, therefore don't expect high quality

